---
layout:     post
title:      "机器人学中的状态估计-04"
subtitle:   "NLNG系统下滤波器 - 1"
date:       2020-12-04
author:     "Yvan"
header-img: "img/in-post/rse-4.jpg"
header-mask: 0.3
no-catalog: false
mathjax: true
tags:
    - SER
typora-root-url: ..
typora-copy-images-to: ..\img\in-post\ser\04
---

# Estimation Machinery

# K4 Nonlinear Non-Gaussian Estimation-1

## 上回总结

>  定义问题 
>
>  -> 批量处理方法的 MAP & 贝叶斯 得到同一个方程 (MAP是模，贝叶斯是均值)
>
>  -> Cholesky分解解方程HWH=LL 前向后向 得到一些关于L的迭代方程组
>
>  -> 继续变形 消去L 变成RTS Smoother（4+1信息形式/5+1经典形式）
>
>  如果不走批量方法，直接走迭代方法
>
>  -> 递归处理方法的 MAP & 贝叶斯
>
>  -> 得到4/5个方程 叫做 卡尔曼滤波器（和RTS中前向部分的一样）



## 这回内容

0. 问题setup
1. [**贝叶斯滤波器**](#Bayes Filter)：基于贝叶斯公式，只是理想的数学模型。
2. [**扩展卡尔曼**](Extended Kalman Filter)：线性化以后代入贝叶斯滤波器。
3. [**高斯滤波器**](#Gaussian Filter)：
4. **迭代扩展卡尔曼**：
5. **粒子滤波器**：

<img src="/img/in-post/ser/04/image-20201230021637961.png" alt="image-20201230021637961" style="zoom:50%;" />

---

### Setup 1

$\frac{b}{x} = \frac{b-v+u}{f+x}$

<img src="/img/in-post/ser/04/image-20201205113520559.png" alt="image-20201205113520559" style="zoom: 33%;" />

**Bayesian**

<img src="/img/in-post/ser/04/image-20201205131301834.png" alt="image-20201205131301834" style="zoom: 67%;" />

**Bayesian Framework**

<img src="/img/in-post/ser/04/image-20201205131938741.png" alt="image-20201205131938741" style="zoom:50%;" />

We start with a prior. The 'true' state is then drawn from the prior, and the measurement is generated by observing the true
state through the camera model and adding noise. The estimator then reconstructs the posterior from the measurement and prior, without knowing x_true.



**MAP**

以前已经说明过：

<img src="/img/in-post/ser/04/image-20201205131324105.png" alt="image-20201205131324105" style="zoom: 80%;" />

for the example of stereo camera:

<img src="/img/in-post/ser/04/image-20201205131601336.png" alt="image-20201205131601336" style="zoom: 56%;" />

<img src="/img/in-post/ser/04/image-20201205131610161.png" alt="image-20201205131610161" style="zoom:56%;" />

so:

<img src="/img/in-post/ser/04/image-20201205132320649.png" alt="image-20201205132320649" style="zoom: 67%;" />

<img src="/img/in-post/ser/04/image-20201205132332336.png" alt="image-20201205132332336" style="zoom:67%;" />

---

### Setup 2

<img src="/img/in-post/ser/04/image-20201205154523619.png" alt="image-20201205154523619" style="zoom:50%;" />

<img src="/img/in-post/ser/04/image-20201205154538245.png" alt="image-20201205154538245" style="zoom:50%;" />

**Markov Property**

the conditional PDF of future states of the process, given the present state, depend only upon the present state, but not on any other past states, i.e., they are conditionally independent of these older states. Such a process is called Markovian or a Markov process.

<img src="/img/in-post/ser/04/image-20201205154851751.png" alt="image-20201205154851751" style="zoom:50%;" />

---

### Bayes Filter

目的：计算  belief function

<img src="/img/in-post/ser/04/image-202012140014087021.png" alt="image-202012140014087021" style="zoom: 50%;" />

**第一步**：用贝叶斯公式展开

<img src="/img/in-post/ser/04/image-20201214001408702.png" alt="image-20201214001408702" style="zoom: 50%;" />

第一步原理：贝叶斯公式：

<center>
$p(x \mid y,z) = $ <font size=5> $\frac{p(y \mid x,z) \cdot p(x \mid z)}{p(y\mid z)} $</font><br/>
here,   $x = x_k \qquad \qquad  y = y_k \qquad \qquad z = x_0, v_1:v_k,y_0:y_{k-1}$<br/>
其中p(y|z)是常数，因为当前的测量只和当前状态有关,和{输入,之前的测量,之前的状态}无关。
 </center>


**第二步**：计算p(x\|z)

<img src="/img/in-post/ser/04/image-20201214002541751.png" alt="image-20201214002541751" style="zoom:50%;" />

第二布原理：全概率公式 + 条件概率公式

<img src="/img/in-post/ser/04/image-20201214003235008.png" alt="image-20201214003235008" style="zoom: 67%;" />

<center>
$p(x，y \mid z) = p(x \mid y,z) \cdot p(y \mid z) $<br/>
 </center>

**第三步**：简化积分中的两项：**hidden state / inverse of marginalization**

<img src="/img/in-post/ser/04/image-20201214003701959.png" alt="image-20201214003701959" style="zoom:50%;" />

第三步原理：马尔可夫性质

当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态。

**第四步**：联立

<img src="/img/in-post/ser/04/image-20201214004019541.png" alt="image-20201214004019541" style="zoom:50%;" />

**可见**：

$x$实现递归推导。

这个状态 = 用g测量得**矫正** 用运动模式f的**预测**  上一个状态

是一个**predictor-corrector**的形式，和卡尔曼滤波器一样。矫正和预测只和**当前**和**上一个状态**有关。

<img src="/img/in-post/ser/04/image-20201214004543113.png" alt="image-20201214004543113" style="zoom:50%;" />

**但是：**

只是一个数学模型，除线性高斯系统中，不可能实践。

1. **infinite amount of memory**: 需要用于储存v1-vk, y0-yk的无限储存空间。
2. **infinite computing resources**: 计算积分的代价很大。

---

### Extended Kalman Filter

先上结论以及与LG系统下的KF的比较：

<img src="/img/in-post/ser/04/image-20201214010531893.png" alt="image-20201214010531893" style="zoom:50%;" />

**EKF是BF的特殊情况（前提/假设）：**

1. prior是高斯分布。

2. 两个噪音w,n是零均值高斯分布，且无关。

3. 运动和观测方程f,g被线性化。（线性化方法在[SER-1-2.2.8](https://yvvvan.github.io/2020/10/26/SER/#228-nonlinear-change)中提到）

4. 运动和观测方程的线性化操作取在如下两个点上：

   <img src="/img/in-post/ser/04/image-20201214012227971.png" alt="image-20201214012227971" style="zoom:50%;" />

**(i) 线性化f,g**

从 [setup 2](#Setup 2) 的运动观测方程开始。

线性化的操作点见上方4

<img src="/img/in-post/ser/04/image-20201229234150062.png" alt="image-20201229234150062" style="zoom: 50%;" />

**(ii) 运动和观测 $p(x_k \| x_{k-1},v_k), p(y_k \| x_k)$**

求p就是计算 x_k,y_k 的均值和协方差

<img src="/img/in-post/ser/04/image-20201230001309931.png" alt="image-20201230001309931" style="zoom: 67%;" />

协方差部分根据定义 w' = 雅可比矩阵 × w， 即 E[w'w'^T] = J·E[ww^T]·J^T = J·Q·J^T = Q'。同样得到n。

**(iii) 代入BF**

上一节推导的BF：

<img src="/img/in-post/ser/04/image-20201214004019541.png" alt="image-20201214004019541" style="zoom:50%;" />

代入后得到：

<img src="/img/in-post/ser/04/image-20201230002557868.png" alt="image-20201230002557868" style="zoom:67%;" />

**(iv) 计算BF积分部分**

积分部分使用 [SER-1-2.2.8](../../../../../../2020/10/26/SER/#228-nonlinear-change) 中的结论: $y \sim \mathcal{N}(\mu_y, \Sigma_{yy}) = \mathcal{N}(g(\mu_x), R+G\Sigma_{xx}G^T)$

将服从⾼斯分布的变量传⼊⾮线性函数中，可以看到积分仍然是⾼斯的。

<img src="/img/in-post/ser/04/image-20201230003730991.png" alt="image-20201230003730991" style="zoom:67%;" />

**(v) 计算整个BF**

需要用到 [SER-1-2.2.6](../../../../../../2020/10/26/SER/#226-normalized-product) 归一化积的结论：

即K 个⾼斯概率密度函数的归⼀化积，仍然是⾼斯概率密度函数。

<img src="/img/in-post/ser/04/image-20201230010312663.png" alt="image-20201230010312663" style="zoom: 67%;" />

在(iv)中两个乘积仍然是高斯的。这一步就变成了求**G1**，**G2**，**μ1**，**μ2**，**Σ1**，**Σ2**。(为了防止字母混淆，这边用加粗表示)

A) (iv)中第二项(积分部分)已经直接说明了$\mathbf{μ}_2=\check{x_k}$ , $\mathbf{Σ}_2 = Q'+FPF = \check{P}_k$ 和 $\mathbf{G}_2 = \mathbf{I}_d$

B) (iv)中第一项展开它的指数部分 得到 $exp(-\frac{1}{2}(y_k-\check{y_k}-G_k(x_k-\check{x_k}))^T(R'_k)^{-1}(...))) $<br/> 化到 **(Gx-μ)Σ$^{-1}$(Gx-μ)**形式。得到$(\mathbf{G}x-\mathbf{μ}) = G_kx_k-(y_k-\check{y_k}+G_k\check{x_k})$ 。即 $\mathbf{μ}_1=y_k-\check{y_k}+G_k\check{x_k}$, $\mathbf{Σ}_1 = R'_k$ 和 $\mathbf{G}_1 = G_k$

代入归一化积结论中。

**(vi) 使用SMW**

**(vii) 定义卡尔曼增益**

解出归一化积中的**μ，Σ**。

<img src="/img/in-post/ser/04/image-20201230014325932.png" alt="image-20201230014325932" style="zoom: 50%;" />



**(viii) 代入整理，得到均值和协方差**

<img src="/img/in-post/ser/04/image-20201230020052300.png" alt="image-20201230020052300" style="zoom: 50%;" />

**结论**

<img src="/img/in-post/ser/04/image-20201214010531893.png" alt="image-20201214010531893" style="zoom:50%;" />

1. 有着和KF相似的结构，两点不同

   1. 非线性的运动和观测模型
   2. 协方差矩阵 Q', R' 中蕴含了雅可比矩阵 J

2. 不能证明对于非线性系统一定有用

   因为线性化的操作点是估计均值点，并非实际均值点。会导致在一些情况中EKF偏离很大，变成有偏的, 不一致的。

---

### Gaussian Filter

---

待续 30.12.2020